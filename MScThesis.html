<!DOCTYPE html>
<html>
<head>
  <title>DatLombardo: M.Sc Thesis</title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <link rel="stylesheet" href="assets/bootstrap.min.css">
  <link rel="stylesheet" type="text/css" href="assets/text.css">
    <link rel="stylesheet" type="text/css" href="assets/thesis.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
</head>

<body>
  <div class="navHead">
      <nav class="navbar navbar-expand-lg navbar-dark bg-primary">
      <a class="navbar-brand" href="index.html">Michael Lombardo</a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarColor01" aria-controls="navbarColor01" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
        <div class="collapse navbar-collapse" id="navbarColor01">
          <ul class="navbar-nav mr-auto">
            <li class="nav-item">
              <a class="nav-link" href="about.html">About Me</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="vitae.html">Curriculum Vitae</a>
            </li>
            <!-- <li class="nav-item">
              <a class="nav-link" href="resume.html">Resume</a>
            </li>-->
            <li class="nav-item">
              <a class="nav-link" href="publications.html">Publications</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="MScThesis.html">M.Sc Thesis</a>
            </li>
            <!-- <li class="nav-item">
              <a class="nav-link" href="socials.html">Social Gallery</a>
            </li> -->
          </ul>
        </div>
    </nav>
  </div>
  <div class="card mb-3">
    <h1 class="card-header" >
      <ol class="breadcrumb">
      <li class="breadcrumb-item"><a href="index.html">Home</a></li>
      <li class="breadcrumb-item">M.Sc Thesis</li>
    </ol>
    </h1>

    <div class="card border-primary mb-3" id="thesisCard" style="max-width: 90%;">
      <div class="card-header" id="thesisTitle">Abstract</div>
      <div class="card-body" id="thesisMaterial">
        <p class="card-text"> Applications of computer vision on natural images have seen great success recently, yet we have seen few approaches dealing with visual illustrations. We propose a collection of computer vision applications for parsing genetic models.  Genetic models are a type of visual illustration often used in the biological sciences literature. These are used to demonstrate how a discovery fits into what is already known about a biological system. A system that determines the interactions present in a genetic model can be valuable to researchers studying such interactions. The proposed system contains three parts. First, a triplet network is deployed to decide whether or not a figure is a genetic model. Second, a popular object detection network YOLOv5 is trained to locate regions of interest within genetic models using various deep learning training techniques. Lastly, we propose an algorithm that can infer the relationships between the pairs of genes or textual features present in the genetic model. Components of this system power the search functionality of the <a href="https://bar.utoronto.ca/gaia/">Bio-Analytic Resource for Plant Biology tool</a> at the University of Toronto.
        </p>

      </div>
    </div>

    <div class="card border-dark mb-3" id="thesisCard" style="max-width: 90%;">
      <div class="card-header" id="thesisTitle">Overview</div>
      <div class="card-body" id="thesisMaterial">
        <p class="card-text">
          Our proposed method is shown below for constructing textual descriptions from a genetic model. It consists of three steps. First, a Triplet classification network is trained on our synthetic dataset named GeneNetSyn, which classifies whether or not an image is a genetic model.  Next, a YOLOv5 model identifies the constituents blocks within genetic models. These blocks consist of genes (geometric shapes enclosing textual elementsof gene or protein names) and activation and inhibition lines between these nodes. It is relatively easy to manually create a set of labeled datasets needed to train the classification network in the first step.  This dataset simply requires images of genetic models andother publication figures paired with a binary label indicating whether or not an image is a genetic model. Our region detection YOLOv5 model in the second step, however, requires richly annotated datasets, which are burdensome to acquire. Instead, the YOLOv5 model, which was initially  trained on natural image datasets, is fine-tuned using synthetic datasets. Furthermore, we employ learning techniques, such as domain adaptation and transfer learning.  Before the third step, the system leverages Google OCR API to extract textual elements from the genetic models. These extracted textual elements, often being genes and proteins, are presented in aconvenient scheme to index the genetic model within the University of Toronto’s BAR. Lastly, the thesis develops an activation/inhibition line analysis system to extract a textual description of the form, “Gene A inhibits Gene B.”
        </p>
        <div class="text-center">
          <div class="col-lg-12">
            <figure class="figure">
            <p><img class="img-responsive" width="50%" src="assets/proposedNet.png" alt=""></a></p>
            <figcaption>
              Proposed solution for diagram parsing on a genetic model displaying desired output.  Broken into three pillars: triplet classification, region detection, and parsing relationships. The goal of this system is to create a textual description of relationships in a genetic model. Genetic model from <a href="https://www.ncbi.nlm.nih.gov/labs/pmc/articles/PMC6686129/">Wang <i>et al.</i></a>
            </figcaption>
            </figure>
          </div>
        </div>
      </div>
    </div>


    <div class="card border-primary mb-3" id="thesisCard" style="max-width: 90%;">
      <div class="card-header" id="thesisTitle">Datasets</div>
      <div class="card-body" id="thesisMaterial">
        <p class="card-text">
          We introduce both the real and synthetic genetic model datasets titled GeneNet and GeneNetSyn, respectively.  Training a deep neural network is a data-dependent task, thus we require large collections of data which is supplemented by richly annotated ground truth.
        </p>
        <h4 class="card-title">GeneNet</h4>
        <p class="card-text">
          Currently, there are no publicly available publication figure datasets in the scope of plant biology. To train a computer vision system for diagram parsing, we require a richly annotated dataset of both publication figures and genetic models. We present the collection of GeneNet datasets that offer ground truth for genetic model classification, region detection, and diagram parsing. GeneNet introduces a new avenue for creating computer vision applications for genetic models analysis.
        </p>
        <div class="text-center">
          <div class="col-lg-12">
            <figure class="figure">
            <p><img class="img-responsive" width="50%" src="assets/gnImgs.png" alt=""></p>
            <figcaption>
              Comparison of the two GeneNet-98 classes. Images shown <a href="https://pubmed.ncbi.nlm.nih.gov/28992087/">Bloomer <i>et al.</i></a>
            </figcaption>
            </figure>
          </div>
        </div>
        <hr>
        <h4 class="card-title">GeneNetSyn</h4>
        <p class="card-text">
          We propose a collection of datasets that iteratively increase in visual complexity. The goal of these synthetic datasets is to attempt to mimic the interaction of genes through relationships present in genetic models from publication figures present in GeneNet.
        </p>
        <div class="container text-center">
          <div class="row">
            <div class="col">
              <figure class="figure">
                <img class="img-responsive" width="50%" src="assets/GNSV3.gif" alt="">
                <figcaption>
                GeneNetSyn Version 3 image
                </figcaption>
              </figure>
            </div>
            <div class="col">
              <figure class="figure">
                <img class="img-responsive" width="50%" src="assets/GNSV3GT.gif" alt="">
                <figcaption>
                GeneNetSyn Version 3 ground truth
                </figcaption>
              </figure>
            </div>
          </div>
        </div>
      </div>
    </div>


    <div class="card border-dark mb-3" id="thesisCard" style="max-width: 90%;">
      <div class="card-header" id="thesisTitle">Approach</div>
      <div class="card-body" id="thesisMaterial">
        <h4 class="card-title">Genetic Model Classification</h4>
        <p class="card-text">
          We introduce our approach to determining whether a given publication figure is a genetic model or not. A Triplet classification network is leveraged to complete this task. A Triplet network requires three input images to decide whether the image of interest (anchor) is more similar to the positive or negative class from the dataset <b>X</b>. For each data item passed to the network, the datais  divided  into  three  sub-batches: the image of interest (anchor) <b>x</b><sub>a</sub>, the  positive class example <b>x</b><sub>p</sub>, and the negative class example <b>x</b><sub>n</sub>.
        </p>
        <div class="text-center">
          <div class="col-lg-12">
            <figure class="figure">
            <p><img class="img-responsive" width="50%" src="assets/tripletNet.png" alt=""></p>
            </figure>
          </div>
        </div>

        <hr>

        <h4 class="card-title">Region Detection</h4>
        <p class="card-text">
          We employ two learning techniques to transfer the weights from Microsoft COCO to our synthetic genetic models within GeneNetSyn. Firstly, we use instance-based mapping transfer learning to tune the weights from the source domain to our target domain. Secondly, to also improve the transition of natural image trained weights to genetic models, we introduce domain adaptation to our region detection networks training approach. Although our region detection networks are being trained on GeneNetSyn, we add real genetic models from the GeneNet datasets to make the transition from synthetic to real genetic models have less impact on performance. A visualization of the YOLOv5 training approach can be seen below.
        </p>
        <div class="text-center">
          <div class="col-lg-12">
            <figure class="figure">
            <p><img class="img-responsive" width="50%" src="assets/regDet.gif" alt=""></p>
            </figure>
          </div>
        </div>

        <hr>

        <h4 class="card-title">Parsing Relationships</h4>
        <p class="card-text">
          We introduce our approach to parsing entities associated with a relationship. We leverage the use of an active contour model. Furthermore, this approach does not use any machine learning inference or prediction. The goal of our task is to generate sets of triplets representing entity-relationship-entities found within a given diagram. The following methodology offers a solution for both GeneNet and GeneNetSyn diagrams. One minor difference between the implementationsof the two datasets is how the text within textual features is collected. For the GeneNetSyn datasets, we assume that text to bounding box association are known. When considering diagrams from GeneNet or other genetic models not known, we would have textual information located using optical character recognition. We can divide our approach into three steps which we will specifically identify and describe: initialization; relationship pairing; and entity-relationship-entity.
        </p>
        <div class="text-center">
          <div class="col-lg-12">
            <figure class="figure">
            <p><img class="img-responsive" width="50%" src="assets/parsing.gif" alt=""></p>
            </figure>
          </div>
        </div>

      </div>
    </div>

</body>
<footer class="page-footer">
  <div class="container-fluid text-center text-md-center">
    <hr>
    <div class="row">
      <div class="col-md-12 mt-md-0 mt-3">
        <h5 class="text">Connect With Me!</h5>
        <div class="d-flex justify-content-center">
          <ul class="social-network social-circle">
              <li><a onclick="location.href='https://www.facebook.com/michael.lombardo.129'" style="cursor: pointer;" class="icoFacebook" title="Facebook"><i class="fa fa-facebook"></i></a></li>
              <li><a onclick="location.href='https://twitter.com/DatLombardo'" style="cursor: pointer;" class="icoTwitter" title="Twitter"><i class="fa fa-twitter"></i></a></li>
              <li><a onclick="location.href='https://www.linkedin.com/in/michael-lombardo-78650a107/'" style="cursor: pointer;" class="icoLinkedin" title="Linkedin"><i class="fa fa-linkedin"></i></a></li>
              <li><a onclick="location.href='https://www.instagram.com/datlombardo/'" style="cursor: pointer;" class="icoInstagram" title="Instagram"><i class="fa fa-instagram"></i></a></li>
              <li><a onclick="location.href='https://github.com/DatLombardo'" style="cursor: pointer;" class="icoGithub" title="GitHub"><i class="fa fa-github"></i></a></li>
          </ul>
        </div>
    </div>
  </div>
  <div class="footer-copyright text-center py-3">Copyright © Michael Lombardo 2021</div>
</footer>
<script src="https://code.jquery.com/jquery-3.5.1.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.min.js" integrity="sha384-w1Q4orYjBQndcko6MimVbzY0tgp4pWB4lZ7lr30WKz0vr/aWKhXdBNmNb5D92v7s" crossorigin="anonymous"></script>
</html>
